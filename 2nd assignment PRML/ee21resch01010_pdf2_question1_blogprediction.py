# -*- coding: utf-8 -*-
"""EE21RESCH01010_PDF2_question1_blogprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QrD0bu7PYMVCQcow51R-NZmogMx6WJTJ

Blog Feedback Problem
NAME-ANNU
ROLL_NO.-EE21RESCH01010

Link to Dataset
http://archive.ics.uci.edu/ml/datasets/BlogFeedback

Importing Packages
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#Functions declaration
    
def closed_form_solution(X,y):
    '''Closed form solution for linear regression'''
    return np.linalg.pinv(X).T@y.T


def polynomial_design_matrix(x, powers, bias=True):
    Phi = np.column_stack((x ** p for p in powers))
    if bias:
        r, _ = Phi.shape
        onevect = np.ones(r)
        Phi = np.column_stack((onevect, Phi))
    return Phi
def with_bias(x,N):
    one_array=np.ones((1,N))
    return np.vstack((one_array,x))
def poly_matrix(x,M):
    if(m==0):
        poly_kernel=np.ones((1,x.shape[1]))
    else:
        poly_kernel=with_bias(x,x.shape[1])
        for i in range(2,M+1):
            poly_kernel=np.vstack((poly_kernel,x**i))
    return poly_kernel

features=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',
        '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23',
        '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',
        '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',
        '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56',
        '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67',
        '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78',
        '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89',
        '90', '91', '92', '93', '94', '95', '96', '97', '98', '99',
        '100', '101', '102', '103', '104', '105', '106', '107', '108',
        '109', '110', '111', '112', '113', '114', '115', '116', '117',
        '118', '119', '120', '121', '122', '123', '124', '125', '126',
        '127', '128', '129', '130', '131', '132', '133', '134', '135',
        '136', '137', '138', '139', '140', '141', '142', '143', '144',
        '145', '146', '147', '148', '149', '150', '151', '152', '153',
        '154', '155', '156', '157', '158', '159', '160', '161', '162',
        '163', '164', '165', '166', '167', '168', '169', '170', '171',
        '172', '173', '174', '175', '176', '177', '178', '179', '180',
        '181', '182', '183', '184', '185', '186', '187', '188', '189',
        '190', '191', '192', '193', '194', '195', '196', '197', '198',
        '199', '200', '201', '202', '203', '204', '205', '206', '207',
        '208', '209', '210', '211', '212', '213', '214', '215', '216',
        '217', '218', '219', '220', '221', '222', '223', '224', '225',
        '226', '227', '228', '229', '230', '231', '232', '233', '234',
        '235', '236', '237', '238', '239', '240', '241', '242', '243',
        '244', '245', '246', '247', '248', '249', '250', '251', '252',
        '253', '254', '255', '256', '257', '258', '259', '260', '261',
        '262', '263', '264', '265', '266', '267', '268', '269', '270',
        '271', '272', '273', '274', '275', '276', '277', '278', '279',
        '280','target']

raw_dataset=pd.read_csv('/content/blogData_train.csv',names=features,na_values="?",sep=",")
dataset = raw_dataset.copy()

print(dataset.shape)

dataset.describe()

dataset.head(3)

#Checking the presence of NA values and dropping them
dataset.isna().sum()

dataset=dataset.dropna() #dropping NA values

correlation_mat=dataset.corr() #FINDING CORRELATION
correlation_mat.describe()

#FEATURE EXTRACTION 

#Selecting only those features which have correlation value greater than or equal to 0.4
cor_target=correlation_mat['target']
cor_target=cor_target[cor_target>=0.4]
cor_target[cor_target>=0.4]

drop_list=['3', '4', '8', '9', 
        '13', '14', '17', '18', '19', '23',
        '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',
        '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',
        '46', '47', '48', '49', '50', '51','53', '54', '55', '56',
        '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67',
        '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78',
        '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89',
        '90', '91', '92', '93', '94', '95', '96', '97', '98', '99',
        '100', '101', '102', '103', '104', '105', '106', '107', '108',
        '109', '110', '111', '112', '113', '114', '115', '116', '117',
        '118', '119', '120', '121', '122', '123', '124', '125', '126',
        '127', '128', '129', '130', '131', '132', '133', '134', '135',
        '136', '137', '138', '139', '140', '141', '142', '143', '144',
        '145', '146', '147', '148', '149', '150', '151', '152', '153',
        '154', '155', '156', '157', '158', '159', '160', '161', '162',
        '163', '164', '165', '166', '167', '168', '169', '170', '171',
        '172', '173', '174', '175', '176', '177', '178', '179', '180',
        '181', '182', '183', '184', '185', '186', '187', '188', '189',
        '190', '191', '192', '193', '194', '195', '196', '197', '198',
        '199', '200', '201', '202', '203', '204', '205', '206', '207',
        '208', '209', '210', '211', '212', '213', '214', '215', '216',
        '217', '218', '219', '220', '221', '222', '223', '224', '225',
        '226', '227', '228', '229', '230', '231', '232', '233', '234',
        '235', '236', '237', '238', '239', '240', '241', '242', '243',
        '244', '245', '246', '247', '248', '249', '250', '251', '252',
        '253', '254', '255', '256', '257', '258', '259', '260', '261',
        '262', '263', '264', '265', '266', '267', '268', '269', '270',
        '271', '272', '273', '274', '275', '276', '277', '278', '279',
        '280']

modified_dataset=dataset.drop(drop_list,axis=1)
#modified_dataset.head()
modified_dataset.shape

#Checking whether these attributes are correlated with each other or not
#To check correlation between attributes we can visualize it from heat map
#lighter ones => high correlation
#darker ones => lower correlation
fig=plt.subplots(figsize=(16,16))
sns.set(font_scale=3)
sns.heatmap(modified_dataset.corr(),square=True,cbar=True,annot=True,annot_kws={'size': 10})

"""USING POLYNOMIAL KERNEL,PERFORMING LINEAR REGRESSION,WE GET"""

#Linear Regression with splitting factor =0.2 ,with only considering greater than 0.4 values
data=modified_dataset.to_numpy()
x=data[:,0:14].T
x=x.astype(float)
t=data[:,15].T
t=t.astype(float)

p=0.2
Xtr=x[:,0:int(p*x.shape[1])]
Ytr=t[0:int(p*x.shape[1])]

M=40
err=np.array([])
pred=[]
for m in range(M+1):
    phi=poly_matrix(Xtr,m)
    weight_vector=closed_form_solution(phi,Ytr)
    y=weight_vector.T@phi
    pred=np.append(pred,y)
    err=np.append(err,np.sqrt(sum(((y-Ytr)**2))/(len(y))))
print("blog Predictions with split factor =0.2",pred)
m_value=np.argmin(err)
min_err=err[m_value]
print("Minimum error occurs for M =",m_value,"and its values is ",min_err)
plt.figure()
plt.plot(err)
plt.title("rms error with respect to model order")
plt.xlabel("m")
plt.ylabel("rms")

#Linear Regression with splitting factor =0.2,all data sets
data=dataset.to_numpy()
x=data[:,0:279].T
x=x.astype(float)
t=data[:,280].T
t=t.astype(float)

p=0.2
Xtr=x[:,0:int(p*x.shape[1])]
Ytr=t[0:int(p*x.shape[1])]
M=10
err=np.array([])
for m in range(M+1):
    phi=poly_matrix(Xtr,m)
    weight_vector=closed_form_solution(phi,Ytr)
    y=weight_vector.T@phi
    
    err=np.append(err,np.sqrt(sum(((y-Ytr)**2))/(len(y))))

m_value=np.argmin(err)
min_err=err[m_value]
print("Minimum error occurs for M =",m_value,"and its values is ",min_err)
plt.figure()
plt.plot(err)
plt.title("rms error with respect to model order")
plt.xlabel("m")
plt.ylabel("rms")

#WITH ALL DATA SETS,NOT RESTRICTING TARGET VALUE GREATER THAN 0.4
#Linear Regression with splitting factor =0.2
data=dataset.to_numpy()
x=data[:,0:279].T

x=x.astype(float)
t1=data[:,280]
t=data[:,280].T
t=t.astype(float)

p=0.2 #SPLIT FACTOR
Xtr=x[:,0:int(p*x.shape[1])]
Ytr=t[0:int(p*x.shape[1])]
M=40
err=np.array([])
for m in range(M+1):
    phi=poly_matrix(Xtr,m)
    weight_vector=closed_form_solution(phi,Ytr)
    y=weight_vector.T@phi
    
    err=np.append(err,np.sqrt(sum(((y-Ytr)**2))/(len(y))))

m_value=np.argmin(err)
min_err=err[m_value]
print("Minimum error occurs for M =",m_value,"and its values is ",min_err)
plt.figure()
plt.plot(err)
plt.title("rms error with respect to model order")
plt.xlabel("m")
plt.ylabel("rms")

#Linear Regression with splitting factor =0.02
data=dataset.to_numpy()
x=data[:,0:279].T
x=x.astype(float)
t=data[:,280].T
t=t.astype(float)

p=0.02 #SPLIT FACTOR
Xtr=x[:,0:int(p*x.shape[1])]
Ytr=t[0:int(p*x.shape[1])]
M=40
err=np.array([])
for m in range(M+1):
    phi=poly_matrix(Xtr,m)
    weight_vector=closed_form_solution(phi,Ytr)
    y=weight_vector.T@phi
    
    err=np.append(err,np.sqrt(sum(((y-Ytr)**2))/(len(y))))

m_value=np.argmin(err)
min_err=err[m_value]
print("Minimum error occurs for M =",m_value,"and its values is ",min_err)
plt.figure()
plt.plot(err)
plt.title("rms error with respect to model order")
plt.xlabel("m")
plt.ylabel("rms")

#Linear Regression with splitting factor =0.001
data=dataset.to_numpy()
x=data[:,0:279].T
x=x.astype(float)
t=data[:,280].T
t=t.astype(float)

p=0.02
Xtr=x[:,0:int(p*x.shape[1])]
Ytr=t[0:int(p*x.shape[1])]
M=40
err=np.array([])
for m in range(M+1):
    phi=poly_matrix(Xtr,m)
    weight_vector=closed_form_solution(phi,Ytr)
    y=weight_vector.T@phi
    
    err=np.append(err,np.sqrt(sum(((y-Ytr)**2))/(len(y))))

m_value=np.argmin(err)
min_err=err[m_value]
print("Minimum error occurs for M =",m_value,"and its values is ",min_err)
plt.figure()
plt.plot(err)
plt.title("rms error with respect to model order")
plt.xlabel("m")
plt.ylabel("rms")

#Linear Regression with splitting factor =0.001
data=dataset.to_numpy()
x=data[:,0:279].T
x=x.astype(float)
t=data[:,280].T
t=t.astype(float)

p=0.001
Xtr=x[:,0:int(p*x.shape[1])]
Ytr=t[0:int(p*x.shape[1])]
M=10
err=np.array([])
pred=[]
for m in range(M+1):
    phi=poly_matrix(Xtr,m)
    weight_vector=closed_form_solution(phi,Ytr)
    y=weight_vector.T@phi
    pred=np.append(pred,y)
    err=np.append(err,np.sqrt(sum(((y-Ytr)**2))/(len(y))))

print("blog predicted value is given by",pred)
m_value=np.argmin(err)
min_err=err[m_value]
print("Minimum error occurs for M =",m_value,"and its values is ",min_err)
plt.figure()
plt.plot(err)
plt.title("rms error with respect to model order")
plt.xlabel("m")
plt.ylabel("rms")


#ERROR IS MINIMUM HERE,SO THIS MODEL IS BEST WITH ALL DATA POINTS

# linear regression using "stochastic" gradient descent 
# function to compute hypothesis / predictions 
def hypothesis(X, theta): 
    return np.dot(X, theta) 
  
# function to compute gradient of error function w.r.t. theta 
def gradient(X, y, theta): 
    h = hypothesis(X, theta) 
    grad = np.dot(X.transpose(), (h - y)) 
    return grad 
  
# function to compute the error for current values of theta 
def cost(X, y, theta): 
    h = hypothesis(X, theta) 
    J = np.dot((h - y).transpose(), (h - y)) 
    J /= 2
    return J[0] 
  
# function to create a list containing various-batches 
def create_diff_batches(X, y, batch_size): 
    mini_batches = [] 
    data = np.hstack((X, y)) 
    np.random.shuffle(data) 
    n_minibatches = data.shape[0] // batch_size 
    i = 0
  
    for i in range(n_minibatches + 1): 
        mini_batch = data[i * batch_size:(i + 1)*batch_size, :] 
        X_mini = mini_batch[:, :-1] 
        Y_mini = mini_batch[:, -1].reshape((-1, 1)) 
        mini_batches.append((X_mini, Y_mini)) 
    if data.shape[0] % batch_size != 0: 
        mini_batch = data[i * batch_size:data.shape[0]] 
        X_mini = mini_batch[:, :-1] 
        Y_mini = mini_batch[:, -1].reshape((-1, 1)) 
        mini_batches.append((X_mini, Y_mini)) 
    return mini_batches 
  
# function to perform batch gradient descent #By default,batch size is 32
def gradientDescent(X, y, learning_rate = 0.001, batch_size = 32): 
    t=X.reshape(2,-1)
    theta = np.zeros((t.shape[1], 1)) 
    error_list = [] 
    max_iters = 100
    for itr in range(max_iters): 
        mini_batches = create_diff_batches(X, y, batch_size) 
        for mini_batch in mini_batches: 
            X_mini, y_mini = mini_batch 
            theta = theta - learning_rate * gradient(X_mini, y_mini, theta) 
            error_list.append(cost(X_mini, y_mini, theta)) 
    return theta, error_list


data=modified_dataset.to_numpy()
x=data[:,0:14].T
x=x.astype(float)
t=data[:,15].T
t=t.astype(float)

p=0.02
Xtr=x[:,0:int(p*x.shape[1])]
Ytr=t[0:int(p*x.shape[1])]
W,ERROR_LIST=gradientDescent(Xtr.ravel(),Ytr,learning_rate = 0.001, batch_size = 32)

